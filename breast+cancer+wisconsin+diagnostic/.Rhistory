y
paste("Square of 5 is", y)
# nest function invocations
mySquare(mySquare(5))
# a function with two arguments
hypotenuse <- function (x, y) {
z_squared <- (x*x + y*y)
return (sqrt(z_squared))
}
# Invoking the function
hypotenuse(3, 4)
paste("Hypotenuse of right triangle with sides 3 and 4 is",
hypotenuse(3, 4))
# Simplify the function with fewer lines
hypotenuse <- function (x, y) {
return (sqrt(x*x + y*y))
}
hypotenuse(3, 4)
paste("Hypotenuse of right triangle with sides 3 and 4 is",
hypotenuse(3, 4))
# Storing the result
z <- hypotenuse(3, 4)
z
paste("Hypotenuse of right triangle with sides 3 and 4 is", z)
# nest function invocations
hypotenuse(hypotenuse(3, 4), 12)
# function with vector input
sumOfOnlyPositive <- function (x) {
return (sum(x[x > 0]))
}
temps <- c(10, -20, 20, -10)
sum(temps)
sumOfOnlyPositive(temps)
sumOfOnlyPositive(c(10, -20, 20, -10))
sumOfAbsolute <- function (x) {
return(sum(abs(x)))
}
sumOfAbsolute(c(10, -20, 20, -10))
abs(temps)
# Boolean functions
isPositive <- function (x) {
return (x > 0)
}
isPositive(10)
isPositive(-10)
isPositive(c(10, -10))
# If one result is needed
allPositive <- function (x) {
return(all(x > 0))
}
allPositive(c(10, 20, 30))
allPositive(c(10, -10, 20))
#
isTeenager <- function (x) {
return (x >= 13 & x <= 19)
}
isTeenager(15)
isTeenager(20)
####----------------------------------------------------------------------------
#### 4.1. Functions and Function Arguments
#
# A function takes zero or more arguments and returns a value.
#   A function with a single argument can be written as shown below.
#   The function is invoked with the required input values.
#   The function can have an explicit return statement.
#   Otherwise, the last statement is evaluated and returned.
inc.1 <- function (x) {
return (x + 1)
}
inc.1(10)
inc.1(x = 10)
inc.1(c(10,20,30))
inc.1(10:20)
inc.1 <- function (x) {
x + 1
}
inc.1(10)
inc.1(x = 10)
inc.1(c(10,20,30))
inc.1(10:20)
# A function with two arguments is shown below.
# When the function is invoked, the parameter names can be explicitly assigned
#   the input values.
# If the input values are named, then the inputs can be provided in any order.
inc.2 <- function (x, y) {
return (x - y)
}
inc.2(10, 20)
inc.2(x = 10, y = 20)
inc.2(y = 20, x = 10)
# The following example shows the mapping of the input values
#   when only one of the arguments is named.
inc.2(10, y = 20)
inc.2(y = 20, 10)
# The above function definitions expect two arguments to be provided
#   when the function is invoked.
# If one or more inputs are missing, the function invocation throws an error.
#   The error messages show that the arguments have no defaults,
#     and hence expect a value to be provided.
inc.2(10)
result <- sayHello()
mySquare <- function (x) {
return(x*x)
}
mySquare(5)
paste("Square of 5 is", mySquare(5))
mySquare(5)
paste("Square of 5 is", mySquare(5))
y <- mySquare(5)
y
paste("Square of 5 is", y)
mySquare(mySquare(5))
hypotenuse <- function (x, y) {
z_squared <- (x*x + y*y)
return (sqrt(z_squared))
}
hypotenuse(3, 4)
paste("Hypotenuse of right triangle with sides 3 and 4 is",
hypotenuse(3, 4))
hypotenuse <- function (x, y) {
return (sqrt(x*x + y*y))
}
hypotenuse(3, 4)
paste("Hypotenuse of right triangle with sides 3 and 4 is",
hypotenuse(3, 4))
z <- hypotenuse(3, 4)
z
paste("Hypotenuse of right triangle with sides 3 and 4 is", z)
hypotenuse(hypotenuse(3, 4), 12)
sumOfOnlyPositive <- function (x) {
return (sum(x[x > 0]))
}
x <- scan(what = character())
sum_of_first_N_odd_squares <- function(n) {
sum <- 0
count <- 0
odd <- 1  # Start with the first odd number
while (count < n) {
sum <- sum + (odd^2)  # Square the odd number and add to sum
odd <- odd + 2  # Move to the next odd number
count <- count + 1
}
return(sum)
}
print(sum_of_first_N_odd_squares(2))  # Output: 10
sum_of_first_N_odd_squares <- function(n) {
sum <- 0
count <- 0
odd <- 1  # Start with the first odd number
while (count < n) {
sum <- sum + (odd^2)  # Square the odd number and add to sum
odd <- odd + 2  # Move to the next odd number
count <- count + 1
}
return(sum)
}
print(sum_of_first_N_odd_squares(2))
print(sum_of_first_N_odd_squares(5))
print(sum_of_first_N_odd_squares(10))
sum_of_first_N_odd_squares_V2 <- function(n) {
odd_numbers <- seq(1, by = 2, length.out = n)  # Generate first n odd numbers
return(sum(odd_numbers^2))  # Square and sum them
}
print(sum_of_first_N_odd_squares_V2(2))
print(sum_of_first_N_odd_squares_V2(5))
print(sum_of_first_N_odd_squares_V2(10))
sum_of_first_N_odd_squares <- function(n) {
sum <- 0
odd <- 1  # Start with the first odd number
for (i in 1:n) {
sum <- sum + (odd^2)  # Square the odd number and add to sum
odd <- odd + 2  # Move to the next odd number
}
return(sum)
}
print(sum_of_first_N_odd_squares(2))
print(sum_of_first_N_odd_squares(5))
print(sum_of_first_N_odd_squares(10))
install.packages("swirl")
packageVersion("swirl")
packageVersion("swirl")
library(swirl)
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent,galton)
regrline <- lm(child ~ parent, galton)
abline(regrline,
| lwd=3, col='red')
abline(regrline, lwd=3, col='red')
summary(regrline)
fit <- lm(child ~ parent, data = galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
residuals <- galton$child - (ols.ic + ols.slope * galton$parent)
lhs - rhs
all.equal(lhs, rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(est(galton$parent))
est <- function(x) ols.ic + ols.slope * x
varEst <- var(est(ols.slope, ols.ic))
varEst <- var(est(galton$parent))
varEst <- var(est(ols.slope, ols.ic))"
varEst <- var(est(ols.slope, ols.ic))
varEst <- var(est(ols.slope, ols.ic))
varEst <- var(est(ols.slope, ols.ic))
varEst <- var(est(ols.slope, ols.ic))
varEst <- var(est(ols.slope, ols.ic))
exit
0
quit
!quit
swirl()
library(swirl)
swirl()
q()
library(swirl)
swirl()
1
swirl()
cor()
View(inc.1)
cor(gpa_nor, gch_nor)
l_norm <- lm(gch_nor ~ gpa_nor)
l_nor <- lm(gch_nor ~ gpa_nor)
fit <- lm(child ~ parent, data = galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs - rhs
all.equal(lhs, rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(est(galton$parent))
varEst <- var(fitted(fit))
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild, varEst + varRes)
efit <- lm(accel ~ mag+dist, attenu)
mean(fit$residuals)
mean(efit$residuals)
fit <- lm(accel ~ mag, data = attenu)
cov(efit$residuals, attenu$mag)
cov(fit$residuals, attenu$dist)
fit <- lm(accel ~ dist, data = attenu)
cov(efit$residuals, attenu$dist)
setwd("~/Desktop/DataScience Tools & Methods /FinalProject")
setwd("~/Desktop/DataScience Tools & Methods /FinalProject")
library(MASS)     # For Linear Discriminant Analysis (LDA)
library(rpart)
library(caret)
data <- read.csv("wdbc.data", header = FALSE)
library(caret)
setwd("~/Desktop/DataScience Tools & Methods /FinalProject")
data <- read.csv("wdbc.data", header = FALSE)
setwd("~/Desktop/DataScience Tools & Methods /FinalProject/breast+cancer+wisconsin+diagnostic")
data <- read.csv("wdbc.data", header = FALSE)
head(data) #View first few rows
dim(data) #Check number of rows and columns
str(data) #Get Column  data types
sum(is.na(data)) # To check missing values
library(MASS)     # For Linear Discriminant Analysis (LDA)
library(rpart)
library(caret)
data <- read.csv("wdbc.data", header = FALSE)
head(data) #View first few rows
dim(data)
str(data)
sum(is.na(data))
colnames(data) <- c("ID", "Diagnosis",
"radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean",
"compactness_mean", "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean",
"radius_se", "texture_se", "perimeter_se", "area_se", "smoothness_se",
"compactness_se", "concavity_se", "concave_points_se", "symmetry_se", "fractal_dimension_se",
"radius_worst", "texture_worst", "perimeter_worst", "area_worst", "smoothness_worst",
"compactness_worst", "concavity_worst", "concave_points_worst", "symmetry_worst", "fractal_dimension_worst")
colnames(data)
data$Diagnosis <- as.factor(data$Diagnosis)
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5)
# Logistic Regression
log_model <- train(Diagnosis ~ ., data = data, method = "glm", family = "binomial", trControl = ctrl)
# Linear Discriminant Analysis (LDA)
lda_model <- train(Diagnosis ~ ., data = data, method = "lda", trControl = ctrl)
# Decision Tree
tree_model <- train(Diagnosis ~ ., data = data, method = "rpart", trControl = ctrl)
cat("Logistic Regression Accuracy:", log_model$results$Accuracy, "\n")
cat("LDA Accuracy:", lda_model$results$Accuracy, "\n")
cat("Decision Tree Accuracy:", tree_model$results$Accuracy, "\n")
accuracy_df <- data.frame(
Model = c("Logistic Regression", "LDA", "Decision Tree"),
Accuracy = c(
log_model$results$Accuracy,
lda_model$results$Accuracy,
mean(tree_model$results$Accuracy)  # in case of multiple values
)
)
library(ggplot2)
ggplot(accuracy_df, aes(x = Model, y = Accuracy, fill = Model)) +
geom_bar(stat = "identity", width = 0.6) +
ylim(0, 1) +
geom_text(aes(label = round(Accuracy, 4)), vjust = -0.5, size = 4.5) +
labs(title = "Classification Model Accuracy Comparison", y = "Accuracy", x = "Model") +
theme_minimal()
library(MASS)     # For LDA
library(rpart)
library(caret)
data <- read.csv("wdbc.data", header = FALSE)
colnames(data) <- c("ID", "Diagnosis",
"radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean",
"compactness_mean", "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean",
"radius_se", "texture_se", "perimeter_se", "area_se", "smoothness_se",
"compactness_se", "concavity_se", "concave_points_se", "symmetry_se", "fractal_dimension_se",
"radius_worst", "texture_worst", "perimeter_worst", "area_worst", "smoothness_worst",
"compactness_worst", "concavity_worst", "concave_points_worst", "symmetry_worst", "fractal_dimension_worst")
data$Diagnosis <- as.factor(data$Diagnosis)
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5)
log_model  <- train(Diagnosis ~ ., data = data, method = "glm",   family = "binomial", trControl = ctrl)
tree_model <- train(Diagnosis ~ ., data = data, method = "rpart", trControl = ctrl)
features_q2 <- c("radius_mean", "texture_mean", "symmetry_mean")
ttest_list <- lapply(features_q2, function(feat) {
t <- t.test(data[[feat]] ~ data$Diagnosis)
data.frame(
Feature   = feat,
Mean_B    = round(t$estimate[1], 3),
Mean_M    = round(t$estimate[2], 3),
t_stat    = round(t$statistic, 3),
p_value   = signif(t$p.value, 3)
)
})
ttest_df <- do.call(rbind, ttest_list)
print(ttest_df)
log_imp  <- varImp(log_model, scale = FALSE)$importance
# Decision tree importance
tree_imp <- varImp(tree_model, scale = FALSE)$importance
log_imp_q2  <- log_imp[features_q2, , drop = FALSE]
tree_imp_q2 <- tree_imp[features_q2, , drop = FALSE]
importance_df <- data.frame(
Feature    = rep(features_q2, 2),
Importance = c(log_imp_q2$Overall, tree_imp_q2$Overall),
Model      = rep(c("Logistic Regression","Decision Tree"), each = length(features_q2))
)
print(importance_df)
library(ggplot2)
ggplot(ttest_df, aes(x = Feature, y = -log10(p_value))) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Strength of Mean Difference (–log10 p‑value)",
y = expression(-log[10](p)), x = "") +
theme_minimal()
ggplot(importance_df, aes(x = Feature, y = Importance, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Feature Importance from Models",
y = "Importance (caret::varImp)", x = "") +
theme_minimal()
ggplot(importance_df, aes(x = Feature, y = Importance, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Feature Importance from Models",
y = "Importance (caret::varImp)", x = "") +
theme_minimal()
ggplot(importance_df, aes(x = Feature, y = Importance, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Feature Importance from Models",
y = "Importance (caret::varImp)", x = "") +
theme_minimal()
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5)
log_model  <- train(Diagnosis ~ ., data = data, method = "glm",   family = "binomial", trControl = ctrl)
tree_model <- train(Diagnosis ~ ., data = data, method = "rpart", trControl = ctrl)
features_q2 <- c("radius_mean", "texture_mean", "symmetry_mean")
ttest_list <- lapply(features_q2, function(feat) {
t <- t.test(data[[feat]] ~ data$Diagnosis)
data.frame(
Feature   = feat,
Mean_B    = round(t$estimate[1], 3),
Mean_M    = round(t$estimate[2], 3),
t_stat    = round(t$statistic, 3),
p_value   = signif(t$p.value, 3)
)
})
ttest_df <- do.call(rbind, ttest_list)
print(ttest_df)
log_imp  <- varImp(log_model, scale = FALSE)$importance
# Decision tree importance
tree_imp <- varImp(tree_model, scale = FALSE)$importance
# Extract our three features
log_imp_q2  <- log_imp[features_q2, , drop = FALSE]
tree_imp_q2 <- tree_imp[features_q2, , drop = FALSE]
importance_df <- data.frame(
Feature    = rep(features_q2, 2),
Importance = c(log_imp_q2$Overall, tree_imp_q2$Overall),
Model      = rep(c("Logistic Regression","Decision Tree"), each = length(features_q2))
)
print(importance_df)
library(ggplot2)
ggplot(importance_df, aes(x = Feature, y = Importance, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Feature Importance from Models",
y = "Importance (caret::varImp)", x = "") +
theme_minimal()
ggplot(ttest_df, aes(x = Feature, y = -log10(p_value))) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Strength of Mean Difference (–log10 p‑value)",
y = expression(-log[10](p)), x = "") +
theme_minimal()
ggplot(importance_df, aes(x = Feature, y = Importance, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Feature Importance from Models",
y = "Importance (caret::varImp)", x = "") +
theme_minimal()
features_q2 <- c("radius_mean", "texture_mean", "symmetry_mean")
ttest_list <- lapply(features_q2, function(feat) {
t <- t.test(data[[feat]] ~ data$Diagnosis)
data.frame(
Feature   = feat,
Mean_B    = round(t$estimate[1], 3),
Mean_M    = round(t$estimate[2], 3),
t_stat    = round(t$statistic, 3),
p_value   = signif(t$p.value, 3)
)
})
ttest_df <- do.call(rbind, ttest_list)
print(ttest_df)
# Logistic regression importance
log_imp  <- varImp(log_model, scale = FALSE)$importance
# Decision tree importance
tree_imp <- varImp(tree_model, scale = FALSE)$importance
# Extract our three features
log_imp_q2  <- log_imp[features_q2, , drop = FALSE]
tree_imp_q2 <- tree_imp[features_q2, , drop = FALSE]
# Combine into one data.frame
importance_df <- data.frame(
Feature    = rep(features_q2, 2),
Importance = c(log_imp_q2$Overall, tree_imp_q2$Overall),
Model      = rep(c("Logistic Regression","Decision Tree"), each = length(features_q2))
)
print(importance_df)
log_imp_q2  <- log_imp[features_q2, , drop = FALSE]
tree_imp_q2 <- tree_imp[features_q2, , drop = FALSE]
importance_df <- data.frame(
Feature    = rep(features_q2, 2),
Importance = c(log_imp_q2$Overall, tree_imp_q2$Overall),
Model      = rep(c("Logistic Regression","Decision Tree"), each = length(features_q2))
)
print(importance_df)
tree_model <- train(Diagnosis ~ radius_mean + texture_mean + symmetry_mean,
data = data, method = "rpart", trControl = ctrl)
features_q2 <- c("radius_mean", "texture_mean", "symmetry_mean")
ttest_list <- lapply(features_q2, function(feat) {
t <- t.test(data[[feat]] ~ data$Diagnosis)
data.frame(
Feature   = feat,
Mean_B    = round(t$estimate[1], 3),
Mean_M    = round(t$estimate[2], 3),
t_stat    = round(t$statistic, 3),
p_value   = signif(t$p.value, 3)
)
})
ttest_df <- do.call(rbind, ttest_list)
print(ttest_df)
# Logistic regression importance
log_imp  <- varImp(log_model, scale = FALSE)$importance
# Decision tree importance
tree_imp <- varImp(tree_model, scale = FALSE)$importance
# Extract our three features
log_imp_q2  <- log_imp[features_q2, , drop = FALSE]
tree_imp_q2 <- tree_imp[features_q2, , drop = FALSE]
# Combine into one data.frame
importance_df <- data.frame(
Feature    = rep(features_q2, 2),
Importance = c(log_imp_q2$Overall, tree_imp_q2$Overall),
Model      = rep(c("Logistic Regression","Decision Tree"), each = length(features_q2))
)
print(importance_df)
ggplot(importance_df, aes(x = Feature, y = Importance, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Feature Importance from Models",
y = "Importance (caret::varImp)", x = "") +
theme_minimal()
log_model  <- train(Diagnosis ~ ., data = data, method = "glm",   family = "binomial", trControl = ctrl)
tree_model <- train(Diagnosis ~ ., data = data, method = "rpart", trControl = ctrl)
features_q2 <- c("radius_mean", "texture_mean", "symmetry_mean")
ttest_list <- lapply(features_q2, function(feat) {
t <- t.test(data[[feat]] ~ data$Diagnosis)
data.frame(
Feature   = feat,
Mean_B    = round(t$estimate[1], 3),
Mean_M    = round(t$estimate[2], 3),
t_stat    = round(t$statistic, 3),
p_value   = signif(t$p.value, 3)
)
})
ttest_df <- do.call(rbind, ttest_list)
print(ttest_df)
# Logistic regression importance
log_imp  <- varImp(log_model, scale = FALSE)$importance
# Decision tree importance
tree_imp <- varImp(tree_model, scale = FALSE)$importance
# Extract our three features
log_imp_q2  <- log_imp[features_q2, , drop = FALSE]
tree_imp_q2 <- tree_imp[features_q2, , drop = FALSE]
# Combine into one data.frame
importance_df <- data.frame(
Feature    = rep(features_q2, 2),
Importance = c(log_imp_q2$Overall, tree_imp_q2$Overall),
Model      = rep(c("Logistic Regression","Decision Tree"), each = length(features_q2))
)
print(importance_df)
